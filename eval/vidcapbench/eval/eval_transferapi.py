#!/usr/bin/env python
# -*- coding: utf-8 -*-
import os
import sys
from pathlib import Path
from transformers import T5Tokenizer
import time
from tqdm import tqdm
import traceback
import json
from json.decoder import JSONDecodeError
from glob import glob
from os import path as osp
import pickle
import re
import argparse
import traceback
import logging
import numpy as np
import requests  

try:
    with open("apikey.txt", "r") as f:
        api_key = f.read().strip()
except:
    api_key = 'sk-iproK7tAwu7J2ZBJWL8G3TiKUepPUH6uj5JQ7w0oXCRu02wl'

base_url = "https://boyuerichdata.chatgptten.com/v1/chat/completions"
headers = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {api_key}"
}

def call_gpt4o(prompt):
    while True:
        try:
            data = {
                "model": "gpt-4o-2024-11-20",
                "messages": [{"role": "user", "content": prompt}],
                "request_timeout": 5
            }
            
            response = requests.post(base_url, headers=headers, json=data)
            
            if response.status_code == 200:
                response_json = response.json()
                output_text = response_json['choices'][0]['message']['content']
                break
            else:
                print(f"请求失败，状态码: {response.status_code}")
                print(f"响应内容: {response.text}")
                time.sleep(5)
                
        except Exception as e:
            print(f"请求超时或错误，正在重试... 错误: {e}")
            time.sleep(5)

    return output_text


def pred_ans(answer_path, QA_path):
    pred_answer_path = answer_path.replace(".jsonl", "_pred_ans.jsonl")
    captions_list = []
    QA_pairs_list = []
    with open(answer_path, 'r') as file:
        for index, line in enumerate(file):
            try:
                captions_list.append(json.loads(line))
            except json.JSONDecodeError as e:
                print(f"Error parsing JSON in file {answer_path}: {e}")

    with open(QA_path, 'r') as file:
        for index, line in enumerate(file):
            try:
                QA_pairs_list.append(json.loads(line))
            except json.JSONDecodeError as e:
                print(f"Error parsing JSON in file {QA_path}: {e}")

    num_caption_lines = len(captions_list)
    num_QA_lines = len(QA_pairs_list) 
    for i in range(num_caption_lines):
        for j in range(num_QA_lines):
            if captions_list[i]["video_dir"] != QA_pairs_list[j]["video_dir"]:
                continue

            model_caption = captions_list[i]["model_generation"]
            QA_pairs = QA_pairs_list[j]["QA_pairs"]

            for index, QA in enumerate(QA_pairs):
                Q = QA["question"]
                prompt = """You are an intelligent chatbot to answer questions given a detailed description of a video or image.
Your answer should be a short sentence or phrase.

Description:
""" + model_caption + f"\n\nQuestion:\n{Q}"
                caption_ans_pred = call_gpt4o(prompt)
                QA_pairs[index]["pred"] = caption_ans_pred

            captions_list[i]["QAPairs"] = json.dumps(QA_pairs, ensure_ascii=False)

            os.makedirs(os.path.dirname(pred_answer_path), exist_ok=True)
            with open(pred_answer_path, 'a') as f:
                json_string = json.dumps(captions_list[i], ensure_ascii=False)
                f.write(json_string + '\n')
    
    return pred_answer_path


def predict_correctness(QA_pairs):
    QA_pairs = json.loads(QA_pairs)
    for index, QAP in enumerate(QA_pairs):
        prompt = '''Please act as an impartial and objective judge and evaluate the correctness of generative outputs for question-answer pairs provided by a Large Language Model.
Your evaluation should be mainly based on whether the predicted answer mentions the provided correct answer comprehensively and accurately.
You need to first comprehensively understand the content of the origin QA pairs and grasp the content of it. Then, you need to analyze if it is accurately reflected in the predicted answer generated by the Large Language Model. For each predicted answer, provide a brief analysis explaining your reasoning for the score.

You will then select from the following options to score the degree to which the model-predicted answer reflects the correct answer:
- Score: 2, the predicted answer comprehensively and accurately reflects the content of the correct answer.
- Score: 1, the predicted answer mentions the correct answer, but the information is not precise or complete. However, there is no contradiction with the correct answer.
- Score: 0, the predicted answer does not mention the correct answer at all.
- Score: -1, the predicted answer contradicts the correct answer or has a partial misrepresentation.
Requirements:
(1) If the predicted answer mentions a subject that is not mentioned in any of the correct answer, and upon reasoning, it is possible that this subject is misidentified from an subject in the correct answer, then prioritize handling it as -1.
(2) When scoring, if the subject in the correct answers is a specific entity, prioritize scoring based on whether the subject is mentioned or if there is a conflict about the subject. Provided that the description of the subject is accurate, then score based on the accuracy of the attributes, states or actions.
(3) If the correct answer doesn't mention a specific entity, Instead, it uses pronouns like "it" or "the person" to refer to subjects, scoring based solely on the accuracy of the attribute, states or actions.
(4) For color attributes, if the caption describes the color of a subject with a specific word that is different from the word used in the key point, but the two colors are similar, then score it as 1.
(5) Please present the result in a JSON dict format: {'score': score, 'analysis': analysis}.

Please help me evaluate whether the predicted answer accurately reflects the correct answer.

''' + f"Question: {QAP['question']}\n" + f"Correct Answer: {QAP['answer']}\n" + f"Predicted Answer: {QAP['pred']}\n" + '''

Your Result:
'''
        judgements = call_gpt4o(prompt)
        if judgements == None:
            continue
        
        if "2" in judgements:
            QA_pairs[index]["correctness"] = "A"
        elif "-1" in judgements:
            QA_pairs[index]["correctness"] = "D"
        elif "1" in judgements:
            QA_pairs[index]["correctness"] = "B"
        elif "0" in judgements:
            QA_pairs[index]["correctness"] = "C"
        else:
            QA_pairs[index]["correctness"] = None

    return QA_pairs


t5_tokenizer = T5Tokenizer.from_pretrained("/mnt/petrelfs/renyiming/ly_workspace/video_benchs/VidCapBench-main/t5")
# t5_tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-base", cache_dir="./tokenizer_cache")

def cal_token_num(caption_path):
    avg_token_num = 0
    video_count = 0
    with open(caption_path, 'r') as file:
        for idx, line in tqdm(enumerate(file)):
            video_count += 1
            data = json.loads(line)
            caption = data["model_generation"]
            caption_ids = t5_tokenizer(caption, return_tensors="pt").input_ids.to("cuda")
            avg_token_num += len(caption_ids[0])
    avg_token_num = float(avg_token_num) / video_count
    return avg_token_num


def cal_cate_correctness(pred_answer_path, pred_correct_path, avg_token_num):
    total_correctness = {"A": 0, "B": 0, "C": 0, "D": 0}
    aesthetics_correctness = {"A": 0, "B": 0, "C": 0, "D": 0}
    content_correctness = {"A": 0, "B": 0, "C": 0, "D": 0}
    motion_correctness = {"A": 0, "B": 0, "C": 0, "D": 0}
    physics_correctness = {"A": 0, "B": 0, "C": 0, "D": 0}

    with open(pred_answer_path, 'r') as file:
        for idx, line in tqdm(enumerate(file)):
            data = json.loads(line)
            QA_pairs = data["QAPairs"]
            new_QA_pairs = predict_correctness(QA_pairs)

            for QA in new_QA_pairs:
                if QA["correctness"] != None:
                    total_correctness[QA["correctness"]] += 1
                    if QA["sub_category"] in ["cinematography", "composition", "lighting", "color", "style",]:
                        aesthetics_correctness[QA["correctness"]] += 1
                    elif QA["sub_category"] in ["background", "subjects", "events",]:
                        content_correctness[QA["correctness"]] += 1
                    elif QA["sub_category"] in ["attribute changes", "object motion", "body movements", "environmental motion", "spatiotemporal transformations",]:
                        motion_correctness[QA["correctness"]] += 1
                    elif QA["sub_category"] in ["spatial relationships", "counter intuitive", "common sense"]:
                        physics_correctness[QA["correctness"]] += 1
                    else:
                        raise ValueError("Category Wrong")

            data["QAPairs"] = new_QA_pairs
            os.makedirs(os.path.dirname(pred_correct_path), exist_ok=True)
            with open(pred_correct_path, 'a') as f:
                json_string = json.dumps(data, ensure_ascii=False)
                f.write(json_string + '\n')
    total_correctness["accuracy"] = 100 * float(total_correctness["A"]) / (total_correctness["A"] + total_correctness["B"] + total_correctness["C"] + total_correctness["D"])
    total_correctness["precision"] = 100 * float(total_correctness["A"] + total_correctness["B"]) / (total_correctness["A"] + total_correctness["B"] + total_correctness["D"])
    total_correctness["coverage"] = 100 * float(total_correctness["A"] + total_correctness["B"] + total_correctness["D"]) / (total_correctness["A"] + total_correctness["B"] + total_correctness["C"] + total_correctness["D"])
    total_correctness["conciseness"] = 100 * total_correctness["accuracy"] / avg_token_num

    aesthetics_correctness["accuracy"] = 100 * float(aesthetics_correctness["A"]) / (aesthetics_correctness["A"] + aesthetics_correctness["B"] + aesthetics_correctness["C"] + aesthetics_correctness["D"])
    aesthetics_correctness["precision"] = 100 * float(aesthetics_correctness["A"] + aesthetics_correctness["B"]) / (aesthetics_correctness["A"] + aesthetics_correctness["B"] + aesthetics_correctness["D"])
    aesthetics_correctness["coverage"] = 100 * float(aesthetics_correctness["A"] + aesthetics_correctness["B"] + aesthetics_correctness["D"]) / (aesthetics_correctness["A"] + aesthetics_correctness["B"] + aesthetics_correctness["C"] + aesthetics_correctness["D"])
    aesthetics_correctness["conciseness"] = 100 * aesthetics_correctness["accuracy"] / avg_token_num

    content_correctness["accuracy"] = 100 * float(content_correctness["A"]) / (content_correctness["A"] + content_correctness["B"] + content_correctness["C"] + content_correctness["D"])
    content_correctness["precision"] = 100 * float(content_correctness["A"] + content_correctness["B"]) / (content_correctness["A"] + content_correctness["B"] + content_correctness["D"])
    content_correctness["coverage"] = 100 * float(content_correctness["A"] + content_correctness["B"] + content_correctness["D"]) / (content_correctness["A"] + content_correctness["B"] + content_correctness["C"] + content_correctness["D"])
    content_correctness["conciseness"] = 100 * content_correctness["accuracy"] / avg_token_num

    motion_correctness["accuracy"] = 100 * float(motion_correctness["A"]) / (motion_correctness["A"] + motion_correctness["B"] + motion_correctness["C"] + motion_correctness["D"])
    motion_correctness["precision"] = 100 * float(motion_correctness["A"] + motion_correctness["B"]) / (motion_correctness["A"] + motion_correctness["B"] + motion_correctness["D"])
    motion_correctness["coverage"] = 100 * float(motion_correctness["A"] + motion_correctness["B"] + motion_correctness["D"]) / (motion_correctness["A"] + motion_correctness["B"] + motion_correctness["C"] + motion_correctness["D"])
    motion_correctness["conciseness"] = 100 * motion_correctness["accuracy"] / avg_token_num

    physics_correctness["accuracy"] = 100 * float(physics_correctness["A"]) / (physics_correctness["A"] + physics_correctness["B"] + physics_correctness["C"] + physics_correctness["D"])
    physics_correctness["precision"] = 100 * float(physics_correctness["A"] + physics_correctness["B"]) / (physics_correctness["A"] + physics_correctness["B"] + physics_correctness["D"])
    physics_correctness["coverage"] = 100 * float(physics_correctness["A"] + physics_correctness["B"] + physics_correctness["D"]) / (physics_correctness["A"] + physics_correctness["B"] + physics_correctness["C"] + physics_correctness["D"])
    physics_correctness["conciseness"] = 100 * physics_correctness["accuracy"] / avg_token_num

    return total_correctness, aesthetics_correctness, content_correctness, motion_correctness, physics_correctness


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--caption_path", type=str, default="/mnt/petrelfs/renyiming/ly_workspace/submit_code/vidcapbench/output/temp.jsonl", help="Path to the captions JSONL file.")
    parser.add_argument("--qa_path", type=str, default="./VidCapBench-AE_QA.jsonl", help="Path to the QA pairs JSONL file.")

    args = parser.parse_args()

    pred_correct_path = args.caption_path.replace(".jsonl", "_pred_correctness.jsonl")
    pred_answer_path = pred_ans(args.caption_path, args.qa_path)
    avg_token_num = cal_token_num(args.caption_path)

    total_correctness, aesthetics_correctness, content_correctness, motion_correctness, physics_correctness = cal_cate_correctness(pred_answer_path, pred_correct_path, avg_token_num)

    save_result_path = pred_answer_path.replace("_pred_ans.jsonl", "_results.log")
    with open(save_result_path, 'w') as f:
        f.write(f"Overall: {total_correctness}\n")
        f.write(f"Video Aesthetics: {aesthetics_correctness}\n")
        f.write(f"Video Content: {content_correctness}\n")
        f.write(f"Video Motion: {motion_correctness}\n")
        f.write(f"Physical Laws: {physics_correctness}\n")